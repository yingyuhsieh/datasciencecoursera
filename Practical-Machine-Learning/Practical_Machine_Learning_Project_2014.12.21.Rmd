Practical Machine Learning Project (2014.12.20)
===
  
The goal of the project is to predict "classe" variable in the training set, create a report describing how to built a model and uuse cross validation, check sample error. The prediction model is going to predict 20 different test cases.  

---  
1. **Setup library and environment**  
    ```{r set_ent, echo=TRUE, results='hide', message=FALSE, warning=FALSE, comment=FALSE, warning=FALSE}
    library(caret)
    library(rattle)
    Sys.setenv("LANGUAGE"="En")
    Sys.setlocale("LC_ALL", "English")
    ```
  
2. **Import data from:**  
    [https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)  
    [https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)
    ```{r read_data, echo=TRUE, cache=TRUE}
    if (!file.exists("pml-training.csv")) {
        download.file(url="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", destfile="pml-training.csv")
    }
    if (!file.exists("pml-testing.csv")) {
        download.file(url="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", destfile="pml-testing.csv")
    }
    pml_training <- read.csv("pml-training.csv")
    pml_testing <- read.csv("pml-testing.csv")
    ```
  
3. **Remove unnecessary columns and highly correlated columns**  
    - Filtering pml_training
    ```{r filter_data_1, echo=TRUE, cache=TRUE}
        ## remove index (1), username (2), timestamp (3,4,5), new_window (6)
        df <- pml_training[, -c(1,2,3,4,5,6)]
    
        ## remove columns with NAs and empty-components greater than 1/10 of row number
        i <- 1:ncol(df)
        v <- sapply(i, function(x){
            (length(which(is.na(df[,x]))) > nrow(df)/10) || (sum(df[,x]=="") > nrow(df)/10)
        })
        df <- df[, !v]
    
        ## set classe as factor
        df$classe <- as.factor(df$classe)
    
        ## remove high correlated columns
        c_i <- which(names(df)=="classe")
        M <- abs(cor(df[,-c_i]))
        h_c_i <- findCorrelation(M, 0.9)
        df <- df[,-h_c_i]
    ```  
    
    - Define training samples and testing samples for modeling with simple cross-validation
    ``` {r setup_training_testing, echo=TRUE, cache=TRUE}
        set.seed(20141221)
        index_cross_training <- createDataPartition(df$classe, p=0.8, list=F)
        cv_training <- df[index_cross_training,]
        cv_testing <- df[-index_cross_training,]
    ```  
    
    - Filtering pml_testing
    ```{r filter_data_2, echo=TRUE, cach=TRUE}
        testing <- pml_testing[,-c(1,2,3,4,5,6)]
        i <- 1:ncol(testing)
        v <- sapply(i, function(x){
            (length(which(is.na(testing[,x]))) > nrow(testing)/10) || (sum(testing[,x]=="") > nrow(testing)/10)
        })
        testing <- testing[,!v] 
    ```  
    
4. **Use Recursive Partitioning and Regression Trees (rpart) model**
    - Build training model
    ```{r model_1, echo=TRUE, cach=TRUE}
    set.seed(20141221)
    modFit1 <- train(classe~., method="rpart", data=cv_training)
    ```  
    
    - Classification tree and accuracy
    ```{r model_1_tree, echo=TRUE, cach=TRUE}
        fancyRpartPlot(modFit1$finalModel)
        ## accurcty of the model
        model_1_accuracy <- max(modFit1$results$Accuracy)
    ```  
    The accuracy of the model is `r sprintf("%.2f %%",  100*max(model_1_accuracy))`
    
    - Cross Validation
    ```{r model_1_cva, echo=TRUE, cach=TRUE}
        model_1_prediction <- predict(modFit1, cv_testing)
        cv_1 <- confusionMatrix(model_1_prediction, cv_testing$classe)
        cv_1
        cv_1_accuracy <- cv_1$overall["Accuracy"]
    ```  
    The cross-validation has accuracy `r sprintf("%.2f %%",  100*max(cv_1_accuracy))`
    
    The performance of **Recursive Partitioning and Regression Trees** (`rpart`) model is poor.
    
5. **Use Random Forest (rf) model**  
    - Build training model
    ```{r model_2, echo=TRUE, cach=TRUE}
    set.seed(20141221)
    tc <- trainControl(method="cv") ## using cross validation
    modFit2 <- train(classe~., method="rf", data=cv_training,
                ntree=200, trControl=tc, allowParallel=T)
    ```
    - Model accuracy
    ```{r model_2_accuracy, echo=TRUE, cach=TRUE}
        ## accurcty of the model
        model_2_accuracy <- max(modFit2$results$Accuracy)
    ```  
    The accuracy of the model is `r sprintf("%.2f %%",  100*max(model_2_accuracy))`
     
     - Cross Validation
    ```{r model_2_cva, echo=TRUE, cach=TRUE}
        model_2_prediction <- predict(modFit2, cv_testing)
        cv_2 <- confusionMatrix(model_2_prediction, cv_testing$classe)
        cv_2
        cv_2_accuracy <- cv_2$overall["Accuracy"]
    ```  
    The cross-validation has accuracy `r sprintf("%.2f %%",  100*max(cv_2_accuracy))`
    
    The performance of **Random Forest** (`rf`) model is great.
       
6. **Predict classe of test data by Random Forest (rf) model**
    ```{r test_data_predict, echo=TRUE, cach=TRUE}
    predict <- predict(modFit2, newdata=pml_testing)
    ```
    The predictions are:  
    `r predict`    
    
    
    
    
    
    
    
    
    
